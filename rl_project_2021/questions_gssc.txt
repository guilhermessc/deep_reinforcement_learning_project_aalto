This document contains Guilherme's answers to the project's questions.

Question 1

TODO: re-write

While DDPG can achieve great performance sometimes, it is frequently brittle with respect to hyperparameters and other kinds of tuning. A common failure mode for DDPG is that the learned Q-function begins to dramatically overestimate Q-values, which then leads to the policy breaking, because it exploits the errors in the Q-function. Twin Delayed DDPG (TD3) is an algorithm that addresses this issue by introducing three critical tricks:

Trick One: Clipped Double-Q Learning. TD3 learns two Q-functions instead of one (hence “twin”), and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.

Trick Two: “Delayed” Policy Updates. TD3 updates the policy (and target networks) less frequently than the Q-function. The paper recommends one policy update for every two Q-function updates.

Trick Three: Target Policy Smoothing. TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.

Together, these three tricks result in substantially improved performance over baseline DDPG.

source: https://spinningup.openai.com/en/latest/algorithms/td3.html



Question 2

TODO: re-write

Exploitation. TD3 trains a deterministic policy in an off-policy way. Because the policy is deterministic, if the agent were to explore on-policy, in the beginning it would probably not try a wide enough variety of actions to find useful learning signals.

As for the algorithm in ex5, it is not deterministic.



Question 3, 4, 5

- implementation-



-------- part 2 --------

Question 1

Both algorithms are designed to give the biggest possible step on the update phase such that the algorithm has a steady trainnig performance. For PPO, clipping the ratio the update should not de-stabilize the training.


Question 2

-implementation-


Question 3

The GAE


